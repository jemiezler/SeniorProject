{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature combinations: 1023\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv(\"resources/color_texture_weight_data.csv\")\n",
    "\n",
    "# Define feature groups\n",
    "feature_groups = {\n",
    "    \"RGB\": [\"Mean_RGB_R\", \"Mean_RGB_G\", \"Mean_RGB_B\", \"Std_RGB_R\", \"Std_RGB_G\", \"Std_RGB_B\"],\n",
    "    \"LAB\": [\"Mean_LAB_L\", \"Mean_LAB_A\", \"Mean_LAB_B\", \"Std_LAB_L\", \"Std_LAB_A\", \"Std_LAB_B\"],\n",
    "    \"HSV\": [\"Mean_HSV_H\", \"Mean_HSV_S\", \"Mean_HSV_V\", \"Std_HSV_H\", \"Std_HSV_S\", \"Std_HSV_V\"],\n",
    "    \"GLCM\": ['GLCM_contrast', 'GLCM_dissimilarity','GLCM_homogeneity', 'GLCM_energy', 'GLCM_correlation', 'GLCM_ASM'],\n",
    "    \"LBP\": ['LBP_0', 'LBP_1', 'LBP_2', 'LBP_3', 'LBP_4', 'LBP_5', 'LBP_6', 'LBP_7', 'LBP_8', 'LBP_9'],\n",
    "    \"Yellow\": [\"Yellow\"],\n",
    "    \"Cyan\": [\"Cyan\"],\n",
    "    \"Magenta\": [\"Magenta\"],\n",
    "    \"Brightness\": [\"Brightness\"],\n",
    "    \"Chroma\": [\"Chroma\"],\n",
    "}\n",
    "\n",
    "\n",
    "feature_comb = []\n",
    "for i in range(1, len(feature_groups) + 1):\n",
    "    for combination in itertools.combinations(feature_groups.keys(), i):\n",
    "        selected_features = [feature for group in combination for feature in feature_groups[group]]\n",
    "        feature_comb.append((combination, selected_features))\n",
    "\n",
    "# Group data by (Rep, Temp) to track weight loss over time\n",
    "grouped_data = df.groupby([\"Rep\", \"Temp\", \"Day\"]).agg({\n",
    "    \"%_Weight_Loss\": \"mean\",\n",
    "    **{feature: \"mean\" for feature in df.columns if feature not in [\"Rep\", \"Temp\", \"Day\", \"Filename\"]}\n",
    "}).reset_index()\n",
    "\n",
    "print(\"Number of feature combinations:\", len(feature_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['%_Weight_Loss', 'Mean_RGB_R', 'Mean_RGB_G', 'Mean_RGB_B', 'Std_RGB_R',\n",
       "       'Std_RGB_G', 'Std_RGB_B', 'Mean_LAB_L', 'Mean_LAB_A', 'Mean_LAB_B',\n",
       "       'Std_LAB_L', 'Std_LAB_A', 'Std_LAB_B', 'Mean_HSV_H', 'Mean_HSV_S',\n",
       "       'Mean_HSV_V', 'Std_HSV_H', 'Std_HSV_S', 'Std_HSV_V', 'Mean_GRAY_Gray',\n",
       "       'Std_GRAY_Gray', 'GLCM_contrast', 'GLCM_dissimilarity',\n",
       "       'GLCM_homogeneity', 'GLCM_energy', 'GLCM_correlation', 'GLCM_ASM',\n",
       "       'LBP_0', 'LBP_1', 'LBP_2', 'LBP_3', 'LBP_4', 'LBP_5', 'LBP_6', 'LBP_7',\n",
       "       'LBP_8', 'LBP_9', 'Day', 'Temp', 'Rep', 'Yellow', 'Cyan', 'Magenta',\n",
       "       'Brightness', 'Chroma'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('resources/color_texture_weight_data.csv')\n",
    "df.drop(columns=['Filename','Weight'], inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_arima(y_train, y_test):\n",
    "    try:\n",
    "        model = ARIMA(y_train, order=(5, 1, 0)).fit()\n",
    "        pred = model.forecast(steps=len(y_test))\n",
    "        return r2_score(y_test, pred)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarima(y_train, y_test):\n",
    "    try:\n",
    "        model = SARIMAX(y_train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7)).fit()\n",
    "        pred = model.forecast(steps=len(y_test))\n",
    "        return r2_score(y_test, pred)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(y_train, y_test):\n",
    "    try:\n",
    "        scaler = MinMaxScaler()\n",
    "        y_train_scaled = scaler.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "        y_test_scaled = scaler.transform(np.array(y_test).reshape(-1, 1))\n",
    "\n",
    "        X_train_lstm, y_train_lstm = y_train_scaled[:-1], y_train_scaled[1:]\n",
    "        X_test_lstm, y_test_lstm = y_test_scaled[:-1], y_test_scaled[1:]\n",
    "\n",
    "        X_train_lstm = X_train_lstm.reshape((X_train_lstm.shape[0], X_train_lstm.shape[1], 1))\n",
    "        X_test_lstm = X_test_lstm.reshape((X_test_lstm.shape[0], X_test_lstm.shape[1], 1))\n",
    "\n",
    "        model = Sequential([\n",
    "            LSTM(50, activation='relu', return_sequences=True, input_shape=(X_train_lstm.shape[1], 1)),\n",
    "            LSTM(50, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
    "        model.fit(X_train_lstm, y_train_lstm, epochs=50, verbose=0, batch_size=8)\n",
    "\n",
    "        pred = model.predict(X_test_lstm)\n",
    "        pred = scaler.inverse_transform(pred)\n",
    "        return r2_score(y_test[1:], pred.flatten())\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_lgb(X_train, X_test, y_train, y_test, model_cls):\n",
    "    try:\n",
    "        model = model_cls(objective=\"reg:squarederror\")\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        return r2_score(y_test, pred)\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results storage\n",
    "results = []\n",
    "best_results = []\n",
    "\n",
    "# Iterate through each (Rep, Temp) group\n",
    "for (rep, temp), sub_df in grouped_data.groupby([\"Rep\", \"Temp\"]):\n",
    "\n",
    "    best_model = None\n",
    "    best_r2 = -np.inf  # Set a low starting value\n",
    "\n",
    "    for feature_group, selected_features in all_feature_combinations:\n",
    "        sub_df = sub_df.sort_values(\"Day\")\n",
    "        X = sub_df[selected_features]\n",
    "        y = sub_df[\"%_Weight_Loss\"]\n",
    "\n",
    "        train_size = int(len(sub_df) * 0.8)\n",
    "        X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "        y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "        model_scores = {\"Rep\": rep, \"Temp\": temp, \"Feature_Group\": feature_group}\n",
    "\n",
    "        # Train models\n",
    "        model_scores[\"ARIMA\"] = train_arima(y_train, y_test)\n",
    "        model_scores[\"SARIMA\"] = train_sarima(y_train, y_test)\n",
    "        model_scores[\"LSTM\"] = train_lstm(y_train, y_test)\n",
    "        model_scores[\"XGBoost\"] = train_xgb_lgb(X_train, X_test, y_train, y_test, xgb.XGBRegressor)\n",
    "        model_scores[\"LightGBM\"] = train_xgb_lgb(X_train, X_test, y_train, y_test, lgb.LGBMRegressor)\n",
    "\n",
    "        # Save results\n",
    "        results.append(model_scores)\n",
    "\n",
    "        # Identify the best model\n",
    "        for model_name in [\"ARIMA\", \"SARIMA\", \"LSTM\", \"Prophet\", \"XGBoost\", \"LightGBM\"]:\n",
    "            if model_scores[model_name] is not None and model_scores[model_name] > best_r2:\n",
    "                best_r2 = model_scores[model_name]\n",
    "                best_model = {**model_scores, \"Best_Model\": model_name, \"Best_R2\": best_r2}\n",
    "\n",
    "    if best_model:\n",
    "        best_results.append(best_model)\n",
    "\n",
    "# Convert results to DataFrame and save\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"forecasting_results.csv\", index=False)\n",
    "\n",
    "best_results_df = pd.DataFrame(best_results)\n",
    "best_results_df.to_csv(\"best_forecasting_results.csv\", index=False)\n",
    "\n",
    "print(\"Forecasting completed! Results saved to 'forecasting_results.csv' and 'best_forecasting_results.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
