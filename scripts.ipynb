{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Picture Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing images\n",
    "FOLDER_PATH = \"resources/images\"  # Change this to your folder path\n",
    "OUTPUT_FILE = \"resources/color_data.csv\"\n",
    "\n",
    "# Define color spaces and their channels\n",
    "COLOR_SPACES = {\n",
    "    \"RGB\": (None, [\"R\", \"G\", \"B\"]),\n",
    "    \"LAB\": (cv2.COLOR_BGR2LAB, [\"L\", \"A\", \"B\"]),\n",
    "    \"HSV\": (cv2.COLOR_BGR2HSV, [\"H\", \"S\", \"V\"]),\n",
    "    \"GRAY\": (cv2.COLOR_BGR2GRAY, [\"Gray\"])\n",
    "}\n",
    "\n",
    "def extract_color(image, conversion_code):\n",
    "    \"\"\" Convert image to specified color space and return mean & std per channel. \"\"\"\n",
    "    img = cv2.cvtColor(image, conversion_code) if conversion_code else image\n",
    "    return np.concatenate([np.mean(img, axis=(0, 1)), np.std(img, axis=(0, 1))]).astype(float) if img.ndim == 3 else [np.mean(img), np.std(img)]\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [f for f in os.listdir(FOLDER_PATH) if f.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff'))]\n",
    "\n",
    "# Process images with a progress bar\n",
    "data = []\n",
    "for file_name in tqdm(image_files, desc=\"Processing Images\", unit=\"image\"):\n",
    "    image = cv2.imread(os.path.join(FOLDER_PATH, file_name))\n",
    "    if image is not None:\n",
    "        row = [file_name] + [val for space, (conv, _) in COLOR_SPACES.itempps() for val in extract_color(image, conv)]\n",
    "        data.append(row)\n",
    "\n",
    "# Generate column names dynamically\n",
    "columns = [\"Filename\"] + [f\"{stat}_{space}_{ch}\" for space, (_, chs) in COLOR_SPACES.items() for stat in [\"Mean\", \"Std\"] for ch in chs]\n",
    "\n",
    "# Save results to CSV\n",
    "pd.DataFrame(data, columns=columns).to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"\\nColor statistics extraction complete! Data saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texture Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern, hog\n",
    "\n",
    "# Folder containing images\n",
    "FOLDER_PATH = \"resources/images\"\n",
    "OUTPUT_FILE = \"resources/texture_data.csv\"\n",
    "\n",
    "# GLCM Features\n",
    "GLCM_PROPS = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']\n",
    "\n",
    "def extract_glcm_features(image_gray):\n",
    "    \"\"\"Extracts GLCM features from grayscale image.\"\"\"\n",
    "    glcm = graycomatrix(image_gray, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n",
    "    return [graycoprops(glcm, prop).flatten()[0] for prop in GLCM_PROPS]\n",
    "\n",
    "def extract_lbp_features(image_gray):\n",
    "    \"\"\"Extracts Local Binary Pattern (LBP) histogram features.\"\"\"\n",
    "    lbp = local_binary_pattern(image_gray, P=8, R=1, method=\"uniform\")\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), range=(0, 10))\n",
    "    return hist.astype(float)\n",
    "\n",
    "def extract_hog_features(image_gray):\n",
    "    \"\"\"Extracts Histogram of Oriented Gradients (HOG) features.\"\"\"\n",
    "    return hog(image_gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True)\n",
    "\n",
    "# Process all images in the folder\n",
    "data = []\n",
    "image_files = [f for f in os.listdir(FOLDER_PATH) if f.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff'))]\n",
    "\n",
    "for file_name in tqdm(image_files, desc=\"Extracting Textures\", unit=\"image\"):\n",
    "    image = cv2.imread(os.path.join(FOLDER_PATH, file_name), cv2.IMREAD_GRAYSCALE)\n",
    "    if image is not None:\n",
    "        glcm_features = extract_glcm_features(image)\n",
    "        lbp_features = extract_lbp_features(image)\n",
    "        hog_features = extract_hog_features(image)[:10]  # Reduce HOG feature size for storage\n",
    "\n",
    "        row = [file_name] + glcm_features + lbp_features.tolist() + hog_features.tolist()\n",
    "        data.append(row)\n",
    "\n",
    "# Generate column names dynamically\n",
    "columns = [\"Filename\"] + [f\"GLCM_{prop}\" for prop in GLCM_PROPS] + [f\"LBP_{i}\" for i in range(10)] + [f\"HOG_{i}\" for i in range(10)]\n",
    "\n",
    "# Save to CSV\n",
    "pd.DataFrame(data, columns=columns).to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"\\nTexture extraction complete! Data saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_color = pd.read_csv('resources/color_data.csv')\n",
    "df_texture = pd.read_csv('resources/texture_data.csv')\n",
    "df_weight = pd.read_csv('resources/weight_loss_data.csv')\n",
    "df_color.sort_values(by='Filename', inplace=True, ignore_index=True)\n",
    "df_texture.sort_values(by='Filename', inplace=True, ignore_index=True)\n",
    "df_weight.sort_values(by='Filename', inplace=True, ignore_index=True)\n",
    "df = pd.merge(df_weight, df_color, on='Filename')\n",
    "df = pd.merge(df, df_texture, on='Filename')\n",
    "\n",
    "    \n",
    "df[['Day', 'Temp', 'Rep']] = df['Filename'].str.extract(r'(\\d+)_(\\d+)_(\\d+)')\n",
    "df[['Day', 'Temp', 'Rep']] = df[['Day', 'Temp', 'Rep']].astype(float).astype('Int64')\n",
    "df[\"Yellow\"] = df[\"Mean_RGB_R\"] + df[\"Mean_RGB_G\"]\n",
    "df[\"Cyan\"] = df[\"Mean_RGB_G\"] + df[\"Mean_RGB_B\"]\n",
    "df[\"Magenta\"] = df[\"Mean_RGB_R\"] + df[\"Mean_RGB_B\"]\n",
    "df[\"Brightness\"] = (df[\"Mean_RGB_R\"] + df[\"Mean_RGB_G\"] + df[\"Mean_RGB_B\"]) / 3\n",
    "df[\"Chroma\"] = df[[\"Mean_RGB_R\", \"Mean_RGB_G\", \"Mean_RGB_B\"]].max(axis=1) - df[[\"Mean_RGB_R\", \"Mean_RGB_G\", \"Mean_RGB_B\"]].min(axis=1)\n",
    "\n",
    "\n",
    "df.drop(columns=['Filename'], inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Weight', '%_Weight_Loss', 'Mean_RGB_R', 'Mean_RGB_G', 'Mean_RGB_B',\n",
       "       'Std_RGB_R', 'Std_RGB_G', 'Std_RGB_B', 'Mean_LAB_L', 'Mean_LAB_A',\n",
       "       'Mean_LAB_B', 'Std_LAB_L', 'Std_LAB_A', 'Std_LAB_B', 'Mean_HSV_H',\n",
       "       'Mean_HSV_S', 'Mean_HSV_V', 'Std_HSV_H', 'Std_HSV_S', 'Std_HSV_V',\n",
       "       'Mean_GRAY_Gray', 'Std_GRAY_Gray', 'GLCM_contrast',\n",
       "       'GLCM_dissimilarity', 'GLCM_homogeneity', 'GLCM_energy',\n",
       "       'GLCM_correlation', 'GLCM_ASM', 'LBP_0', 'LBP_1', 'LBP_2', 'LBP_3',\n",
       "       'LBP_4', 'LBP_5', 'LBP_6', 'LBP_7', 'LBP_8', 'LBP_9', 'Day', 'Temp',\n",
       "       'Rep', 'Yellow', 'Cyan', 'Magenta', 'Brightness', 'Chroma'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subset sizes:   0%|          | 0/11 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Subset sizes:   9%|‚ñâ         | 1/11 [00:00<00:06,  1.59it/s]\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Subset sizes:  18%|‚ñà‚ñä        | 2/11 [00:05<00:27,  3.10s/it]\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A/home/jemiezler/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "\u001b[A\n",
      "Subset sizes:  18%|‚ñà‚ñä        | 2/11 [00:06<00:28,  3.18s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 115\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Another tqdm for the models\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m tqdm(models\u001b[38;5;241m.\u001b[39mitems(), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 115\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ‚úÖ Train on scaled data\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)  \u001b[38;5;66;03m# ‚úÖ Predict on scaled data\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     metric_values \u001b[38;5;241m=\u001b[39m calc_metrics(y_test, y_pred)\n",
      "File \u001b[0;32m~/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/svm/_classes.py:582\u001b[0m, in \u001b[0;36mLinearSVR.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    578\u001b[0m penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# SVR only accepts l2 penalty\u001b[39;00m\n\u001b[1;32m    580\u001b[0m _dual \u001b[38;5;241m=\u001b[39m _validate_dual_parameter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, penalty, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m\"\u001b[39m, X)\n\u001b[0;32m--> 582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43m_fit_liblinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_dual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# Backward compatibility: _fit_liblinear is used both by LinearSVC/R\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# and LogisticRegression but LogisticRegression sets a structured\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;66;03m# `n_iter_` attribute with information about the underlying OvR fits\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# while LinearSVC/R only reports the maximum value.\u001b[39;00m\n",
      "File \u001b[0;32m~/Dev/Kale-SeniorProject/venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1229\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m   1226\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m   1228\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[0;32m-> 1229\u001b[0m raw_coef_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mliblinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_wrap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;66;03m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;66;03m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;66;03m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;66;03m# srand supports\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m n_iter_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n_iter_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    import itertools\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from joblib import dump\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    from sklearn.linear_model import (\n",
    "        LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge,\n",
    "        ARDRegression, HuberRegressor, RANSACRegressor,\n",
    "        PassiveAggressiveRegressor\n",
    "    )\n",
    "    from sklearn.svm import LinearSVR\n",
    "\n",
    "    # For the progress bars\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # ======================\n",
    "    # 1) Define your feature groups\n",
    "    # ======================\n",
    "    features = {\n",
    "        \"Lab\": [\n",
    "            \"Mean_LAB_L\", \"Std_LAB_L\", \"Mean_LAB_A\", \"Std_LAB_A\", \"Mean_LAB_B\", \"Std_LAB_B\"\n",
    "        ],\n",
    "        \"HSV\": [\n",
    "            \"Mean_HSV_H\", \"Std_HSV_H\", \"Mean_HSV_S\", \"Std_HSV_S\", \"Mean_HSV_V\", \"Std_HSV_V\"\n",
    "        ],\n",
    "        \"RGB\": [\n",
    "            \"Mean_RGB_R\", \"Std_RGB_R\", \"Mean_RGB_G\", \"Std_RGB_G\", \"Mean_RGB_B\", \"Std_RGB_B\"\n",
    "        ],\n",
    "        \"GLCM\": [\n",
    "            \"GLCM_contrast\", \"GLCM_dissimilarity\", \"GLCM_homogeneity\", \n",
    "            \"GLCM_energy\", \"GLCM_correlation\"\n",
    "        ],\n",
    "        \"LBP\": [\n",
    "            \"LBP_0\", \"LBP_1\", \"LBP_2\", \"LBP_3\", \"LBP_4\", \"LBP_5\", \"LBP_6\", \"LBP_7\"\n",
    "        ],\n",
    "        \"Temp\": [\"Temp\"],\n",
    "        \"Yellow\": [\"Yellow\"],\n",
    "        \"Cyan\": [\"Cyan\"],\n",
    "        \"Magenta\": [\"Magenta\"],\n",
    "        \"Brightness\": [\"Brightness\"],\n",
    "        \"Chroma\": [\"Chroma\"],\n",
    "    }\n",
    "\n",
    "    # ======================\n",
    "    # 2) Define your models\n",
    "    # ======================\n",
    "    models = {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"Ridge\": Ridge(),\n",
    "        \"Lasso\": Lasso(),\n",
    "        \"ElasticNet\": ElasticNet(),\n",
    "        \"BayesianRidge\": BayesianRidge(),\n",
    "        \"ARDRegression\": ARDRegression(),\n",
    "        \"HuberRegressor\": HuberRegressor(),\n",
    "        \"RANSACRegressor\": RANSACRegressor(),\n",
    "        \"PassiveAggressiveRegressor\": PassiveAggressiveRegressor(),\n",
    "        \"LinearSVR\": LinearSVR(),\n",
    "    }\n",
    "\n",
    "    # A helper function for metrics (we'll compute R¬≤ and MSE)\n",
    "    def calc_metrics(y_true, y_pred):\n",
    "        return {\n",
    "            \"R2\": r2_score(y_true, y_pred),\n",
    "            \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "        }\n",
    "\n",
    "    # 3) Your DataFrame 'df' must have all columns from `features` plus this target column\n",
    "    # df = pd.read_csv(\"your_data.csv\")\n",
    "    target_column = \"%_Weight_Loss\"  # make sure this column exists\n",
    "\n",
    "    # Create a directory to store saved models\n",
    "    model_dir = \"output/all/saved_models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # We'll collect all results in a list of dicts ‚Üí DataFrame\n",
    "    results = []\n",
    "\n",
    "    # Extract the keys in a list\n",
    "    feature_keys = list(features.keys())\n",
    "    num_keys = len(feature_keys)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 4) For r = 1..N, get all combinations of that size\n",
    "    #    and track progress with tqdm\n",
    "    # -----------------------------------------------------------\n",
    "    for r in tqdm(range(1, num_keys + 1), desc=\"Subset sizes\"):\n",
    "        # Combinations of feature_groups of size r\n",
    "        combos = list(itertools.combinations(feature_keys, r))\n",
    "        \n",
    "        # Use a nested tqdm for the combos themselves\n",
    "        for combo in tqdm(combos, desc=f\"Combos of size {r}\", leave=False):\n",
    "            # Flatten the columns from these groups\n",
    "            selected_cols = []\n",
    "            for k in combo:\n",
    "                selected_cols.extend(features[k])\n",
    "            \n",
    "            # X = selected columns, y = target\n",
    "            X = df[selected_cols]\n",
    "            y = df[target_column]\n",
    "            \n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42\n",
    "            )\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Combo name like \"Lab+HSV\" or \"Lab+HSV+RGB\", etc.\n",
    "            combo_str = \"+\".join(combo)\n",
    "            \n",
    "            # Another tqdm for the models\n",
    "            for model_name, model in tqdm(models.items(), desc=\"Models\", leave=False):\n",
    "                model.fit(X_train_scaled, y_train)  # ‚úÖ Train on scaled data\n",
    "                y_pred = model.predict(X_test_scaled)  # ‚úÖ Predict on scaled data\n",
    "                \n",
    "                metric_values = calc_metrics(y_test, y_pred)\n",
    "                \n",
    "                row = {\n",
    "                    \"SubsetSize\": r,\n",
    "                    \"Feature Groups\": combo_str,\n",
    "                    \"Model\": model_name,\n",
    "                    \"R2\": metric_values[\"R2\"],\n",
    "                    \"MSE\": metric_values[\"MSE\"],\n",
    "                }\n",
    "                results.append(row)\n",
    "                \n",
    "                # Save model\n",
    "                model_filename = f\"{model_dir}/model_{model_name}_{combo_str}.joblib\"\n",
    "                dump(model, model_filename)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 5) Convert results to DataFrame & Save to CSV\n",
    "    # ------------------------------------------------\n",
    "    results_df = pd.DataFrame(results)\n",
    "    # Reorder columns if desired\n",
    "    results_df = results_df[[\"SubsetSize\", \"Feature Groups\", \"Model\", \"R2\", \"MSE\"]]\n",
    "\n",
    "    print(\"\\nComplete Results:\\n\", results_df.head(20), \" ...\\n\")  # Show top 20 rows as a preview\n",
    "\n",
    "    csv_filename = \"output/feature_results.csv\"\n",
    "    results_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"\\nSaved to '{csv_filename}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import joblib  # For saving the best model\n",
    "from tqdm import tqdm  # Progress tracking\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, ARDRegression, \n",
    "    HuberRegressor, RANSACRegressor, PassiveAggressiveRegressor\n",
    ")\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset (Ensure df is properly loaded before running)\n",
    "# df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "features = {\n",
    "    \"Lab\": [\"L_Mean\", \"L_Std\", \"a_Mean\", \"a_Std\", \"b_Mean\", \"b_Std\"],\n",
    "    \"HSV\": [\"H_Mean\", \"H_Std\", \"S_Mean\", \"S_Std\", \"V_Mean\", \"V_Std\"],\n",
    "    \"RGB\": [\"R_Mean\", \"R_Std\", \"G_Mean\", \"G_Std\", \"B_Mean\", \"B_Std\"],\n",
    "    \"GLCM\": [\"GLCM_Contrast\", \"GLCM_Dissimilarity\", \"GLCM_Homogeneity\", \"GLCM_Energy\", \"GLCM_Correlation\"],\n",
    "    \"LBP\": [\"LBP_0\", \"LBP_1\", \"LBP_2\", \"LBP_3\", \"LBP_4\", \"LBP_5\", \"LBP_6\", \"LBP_7\"],\n",
    "    # \"Day\": [\"Day\"],\n",
    "    \"Temp\": [\"Temp\"],\n",
    "    \"Yellow\": [\"Yellow\"],\n",
    "    \"Cyan\": [\"Cyan\"],\n",
    "    \"Magenta\": [\"Magenta\"],\n",
    "    \"Brightness\": [\"Brightness\"],\n",
    "    \"Chroma\": [\"Chroma\"],\n",
    "}\n",
    "\n",
    "# Target variable\n",
    "target_column = \"%_Weight_Loss\"\n",
    "\n",
    "# Ensure the target column exists\n",
    "if target_column not in df.columns:\n",
    "    raise KeyError(f\"Target column '{target_column}' not found in dataset.\")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"BayesianRidge\": BayesianRidge(),\n",
    "    \"ARDRegression\": ARDRegression(),\n",
    "    \"HuberRegressor\": HuberRegressor(),\n",
    "    \"RANSACRegressor\": RANSACRegressor(),\n",
    "    \"PassiveAggressiveRegressor\": PassiveAggressiveRegressor(),\n",
    "    \"LinearSVR\": LinearSVR(),\n",
    "}\n",
    "\n",
    "# Get unique temperature values\n",
    "temperatures = df[\"Temp\"].unique()\n",
    "\n",
    "# Store best models per temperature\n",
    "best_models_temp = {temp: {name: {\"r2\": -np.inf, \"model\": None, \"features\": None} for name in models} for temp in temperatures}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Count total combinations for tqdm progress bar\n",
    "total_combinations = sum(1 for _ in itertools.chain.from_iterable(\n",
    "    itertools.combinations(features.keys(), r) for r in range(1, len(features) + 1)\n",
    ")) * len(models) * len(temperatures)\n",
    "\n",
    "progress_bar = tqdm(total=total_combinations, desc=\"Training Models\")\n",
    "\n",
    "# Train models per temperature\n",
    "for temp in temperatures:\n",
    "    temp_df = df[df[\"Temp\"] == temp]\n",
    "    \n",
    "    for r in range(1, len(features)):\n",
    "        for selected_groups in itertools.combinations(features.keys(), r):\n",
    "            selected_features = [col for group in selected_groups for col in features[group] if col in temp_df.columns]\n",
    "            \n",
    "            if not selected_features:\n",
    "                continue\n",
    "            \n",
    "            X = temp_df[selected_features].copy()\n",
    "            y = temp_df[target_column].copy()\n",
    "            \n",
    "            # Handle missing values\n",
    "            X = X.fillna(X.median())\n",
    "            y = y.fillna(y.median())\n",
    "            \n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            \n",
    "            # Standardize the features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Train and evaluate each model\n",
    "            for model_name, model in models.items():\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                \n",
    "                # Evaluate model\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                \n",
    "                # Store results\n",
    "                results.append({\n",
    "                    \"Temperature\": temp,\n",
    "                    \"Model\": model_name,\n",
    "                    \"Feature_Groups\": selected_groups,\n",
    "                    \"MAE\": mae,\n",
    "                    \"MSE\": mse,\n",
    "                    \"R¬≤ Score\": r2\n",
    "                })\n",
    "                \n",
    "                # Update best model for this temperature\n",
    "                if r2 > best_models_temp[temp][model_name][\"r2\"]:\n",
    "                    best_models_temp[temp][model_name] = {\"r2\": r2, \"model\": model, \"features\": selected_features}\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.sort_values(by=[\"Temperature\", \"Model\", \"R¬≤ Score\"], ascending=[True, True, False], inplace=True)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(\"output/linear_model_comparison_per_temp.csv\", index=False)\n",
    "\n",
    "# Save best models per temperature\n",
    "for temp, models_info in best_models_temp.items():\n",
    "    for model_name, best_info in models_info.items():\n",
    "        if best_info[\"model\"] is not None:\n",
    "            model_filename = f\"output/best_{model_name.lower()}_temp_{temp}.pkl\"\n",
    "            joblib.dump(best_info[\"model\"], model_filename)\n",
    "            print(f\"‚úÖ Best {model_name} model for Temp={temp} saved as '{model_filename}' with R¬≤ Score: {best_info['r2']:.4f}\")\n",
    "            print(f\"   ‚úÖ Best feature set: {best_info['features']}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nüîç Top 10 Model Comparisons Across Temperatures:\")\n",
    "print(results_df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
